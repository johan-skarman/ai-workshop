{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. LangChain basics - Messages, Prompts, Output Parsers and Chains\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/IT-HUSET/ai-workshop-250121/blob/main/lab/1-langchain-basics.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>"
   ],
   "id": "71812daf78b1d9e1"
  },
  {
   "cell_type": "markdown",
   "id": "fabe13a58b126b36",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pydantic~=2.9 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 --upgrade --quiet\n",
    "%pip install langchain-anthropic~=0.3.3 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6b4cb397887d64",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "221169915536536a",
   "metadata": {},
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e7a89e07a87f543d",
   "metadata": {},
   "source": "### Setup Models"
  },
  {
   "cell_type": "code",
   "id": "bb6077d675b1cca",
   "metadata": {},
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o-mini\", temperature=0.0, openai_api_version=api_version)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LangChain basics\n",
    "\n",
    "This notebook introduces some of the basics concepts of LangChain.\n",
    "\n",
    "![LanChain](https://raw.githubusercontent.com/IT-HUSET/ai-workshop-250121/refs/heads/main/images/LangChain-chains.png)\n",
    "\n",
    "\n",
    "### Chain, LCEL and the Runnable interface\n",
    "\n",
    "A chain is a sequence of components with a unified interface, that are executed in order. This unified interface is called **[`Runnable`](https://python.langchain.com/docs/concepts/runnables/)** and provides common operations,  **invoking**, **streaming** and **batching** .\n",
    "\n",
    "Multiple Runnables can be composed into a chain, where the output of one Runnable is passed as input to the next Runnable in the chain. The easiest way of doing this is by using the [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/), which basically simply is some syntactic sugar that allows components to be composed together using the `|` operator.\n",
    "\n",
    "```python\n",
    "chain = runnable1 | runnable2\n",
    "```\n",
    "\n",
    "\n",
    "The output of one runnable is passed as input to the next runnable in the chain.\n",
    "https://python.langchain.com/docs/concepts/lcel/\n",
    "\n",
    "\n",
    "### Chat models\n",
    "LangChain provides a consistent interface for working with chat models from different providers. Read more [here](https://python.langchain.com/docs/concepts/chat_models/).\n",
    "\n",
    "\n",
    "### Messages\n",
    "\n",
    "Messages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
    "\n",
    "![Graph](https://github.com/IT-HUSET/ai-workshop-250121/blob/main/images/langchain-messages.png?raw=true)\n",
    "\n",
    "Read more about messages [here](https://python.langchain.com/docs/concepts/messages/).\n",
    "\n",
    "\n",
    "### Output parsing\n",
    "\n",
    "Output parsers are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\n",
    "\n",
    "Read more [here](https://python.langchain.com/docs/concepts/output_parsers/)\n",
    "\n",
    "\n",
    "### More LangChain concepts\n",
    "\n",
    "Read more about basic LangChain concepts [here](https://python.langchain.com/docs/concepts/).\n",
    "\n"
   ],
   "id": "351aa4f0adb24f4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Let's start Chat models and Messages\n",
    "\n",
    "We define a system message and a human message to start a conversation"
   ],
   "id": "2d17a60b1998a7e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "\n",
    "# We define a system message and a human message to start a conversation\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant, expert in Iceland tourist information.\")\n",
    "human_message = HumanMessage(content=\"Hi! I need help planning a trip to Iceland.\")"
   ],
   "id": "a3b9ed187731be40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Since Chat Models are Runnables, we can invoke them using the `invoke` method",
   "id": "60f2c39b6954ca81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ai_message: AIMessage = llm.invoke([system_message, human_message])\n",
    "print(ai_message) # This will (basically) print the entire response from the LLM, including a lot of meta-data, metrics, etc."
   ],
   "id": "96c75dd8a9e44188",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print just the content of the AI message\n",
    "print(ai_message.content)"
   ],
   "id": "854cfa848a7e3f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Conversation",
   "id": "f3fdf2314c61ad35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's try a follow-up question\n",
    "conversation_messages: list[BaseMessage] = [\n",
    "    system_message,\n",
    "    human_message,\n",
    "    ai_message,\n",
    "    HumanMessage(content=\"What if there's a volcanic eruption!?ðŸ˜±\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(conversation_messages)\n",
    "print(response.content)"
   ],
   "id": "9118a0084385c73e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try with a different LLM",
   "id": "63cb46e69c31b014"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm2 = ChatAnthropic(\n",
    "     model='claude-3-5-sonnet-20241022',\n",
    "     temperature=0.0,\n",
    ")\n",
    "\n",
    "response = llm2.invoke(conversation_messages)\n",
    "print(response.content)"
   ],
   "id": "71b63b354e36cd22",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1374dfefa96c8e0e",
   "metadata": {},
   "source": [
    "----\n",
    "<br/>\n",
    "\n",
    "## Prompts and Prompt Templates"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chat prompt template - for chat-based LLMs\n",
    "Basically a chat prompt template is a list of message templates. The result of invoking a chat prompt template is a `ChatPromptValue`, containing a list of messages.\n",
    "\n",
    "```python\n",
    "ChatPromptValue(\n",
    "    messages=[\n",
    "        SystemMessage(content='You are a helpful AI bot. Your name is Carl.'),\n",
    "        HumanMessage(content='Hello, there!'),\n",
    "    ]\n",
    ")\n",
    "```"
   ],
   "id": "3e1153c1ca06ac96"
  },
  {
   "cell_type": "code",
   "id": "87b67cd139c66b26",
   "metadata": {},
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_template = \"Translate user input into a style that is {style}.\"\n",
    "\n",
    "# This is the easiest and most common way to create a prompt template\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{input}\"), # You can also use the alias \"user\" instead of \"human\"\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Let's check the messages templates",
   "id": "b46c3fcc82fc68e8"
  },
  {
   "cell_type": "code",
   "id": "1a3ac2d0f7576987",
   "metadata": {},
   "source": "print(prompt_template.messages)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Print input variables",
   "id": "4afdefe947f0e4f9"
  },
  {
   "cell_type": "code",
   "id": "a92db1765d6e1812",
   "metadata": {},
   "source": "print(prompt_template.input_variables)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using a prompt template",
   "id": "eb37b9f541c7aabd"
  },
  {
   "cell_type": "code",
   "id": "83125e5e1063efa5",
   "metadata": {},
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "\n",
    "customer_style = \"American English in a calm and respectful tone\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_messages: ChatPromptValue = prompt_template.invoke({'style': customer_style, 'input': customer_email})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Let's have a look at the contents (messages):",
   "id": "2a055b1168165e9d"
  },
  {
   "cell_type": "code",
   "id": "d3872fdb49b8c292",
   "metadata": {},
   "source": [
    "# First (system) message:\n",
    "print(type(customer_messages.messages[0]))\n",
    "print(customer_messages.messages[0].content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Second (human) message:\n",
    "print(type(customer_messages.messages[1]))\n",
    "print(customer_messages.messages[1].content)"
   ],
   "id": "f992943db614e2e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acb93ea6d27b4d86",
   "metadata": {},
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = llm.invoke(customer_messages)\n",
    "print(customer_response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Try another example",
   "id": "d53fe42c8338e409"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "customer_style = \"The same style as the user\"\n",
    "\n",
    "customer_messages: ChatPromptValue = prompt_template.invoke({'style': customer_style, 'input': customer_email})\n",
    "\n",
    "customer_response = llm.invoke(customer_messages)\n",
    "print(customer_response.content)"
   ],
   "id": "20d54b3d2434415b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d71df032274e8c0",
   "metadata": {},
   "source": [
    "----\n",
    "<br/>\n",
    "\n",
    "\n",
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236fa5ba4402834f",
   "metadata": {},
   "source": [
    "## The most common parser - String output parsing\n",
    "\n",
    "Useful when you just want to extract a string (content) from the output, and not all the other metadata an LLM might return."
   ]
  },
  {
   "cell_type": "code",
   "id": "f77f921dc68d9e94",
   "metadata": {},
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "response = llm.invoke(\"Copenhagen or Many-worlds?\")\n",
    "print(response) # Will print out the entire response, a lot of which we don't need"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's just extract the content\n",
    "parsed_response = str_parser.invoke(response)\n",
    "print(parsed_response)"
   ],
   "id": "58dab2c794038e7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8bfd095488b3ca5c",
   "metadata": {},
   "source": [
    "## Structured output parsing\n",
    "\n",
    "![Stryctyred output](https://python.langchain.com/assets/images/structured_output-2c42953cee807dedd6e96f3e1db17f69.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### This is what we'd like the output to look like:",
   "id": "7661745c1102696a"
  },
  {
   "cell_type": "code",
   "id": "df8870b805fae9f6",
   "metadata": {},
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"pretty affordable!\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setup the inputs and prompt template",
   "id": "5517db3c5da4b947"
  },
  {
   "cell_type": "code",
   "id": "dbe9c9d9379bd12c",
   "metadata": {},
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any quote about the value or price.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ef1d624b1e52f69",
   "metadata": {},
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Let's try it out",
   "id": "cb82378a35fe3275"
  },
  {
   "cell_type": "code",
   "id": "ef70678c44331c54",
   "metadata": {},
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n",
    "print(f\"\\nResponse type: {type(response.content)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b10d3c0df23f733",
   "metadata": {},
   "source": [
    "### Parse the LLM output as JSON\n",
    "\n",
    "Let's fix this with proper JSON parsing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "result = json_parser.invoke(response)\n",
    "print(result)\n",
    "print(f\"\\nResponse type: {type(result)}\")"
   ],
   "id": "d75eec4c09374bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Structured parsing with typing and optional validation (using **Pydantic**)",
   "id": "edfc1c3f772437ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Review(BaseModel):\n",
    "    gift: bool\n",
    "    delivery_days: int\n",
    "    price_value: str\n",
    "\n",
    "\n",
    "structured_output_llm = llm.with_structured_output(Review)\n",
    "result = structured_output_llm.invoke(messages)\n",
    "\n",
    "print(result)\n",
    "print(f\"\\nResponse type: {type(result)}\")\n",
    "print(f\"Delivery days: {result.delivery_days}\")\n"
   ],
   "id": "5fa89de2cb166fa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "<br/>\n",
    "\n",
    "## Chains"
   ],
   "id": "f248d5bc4ed3fbcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Chain",
   "id": "8018a3b1f2cc174a"
  },
  {
   "metadata": {
    "height": 98
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# When using only a single template string, it's assumed the role is \"human\"\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "output_parser = StrOutputParser()"
   ],
   "id": "8fa3e907eff8d09c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "height": 30
   },
   "cell_type": "code",
   "source": [
    "# Build a chain (creates a RunnableSequence)\n",
    "chain = prompt | llm | output_parser"
   ],
   "id": "997c85d9470362fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "height": 30
   },
   "cell_type": "code",
   "source": "chain.invoke({\"topic\": \"bears\"})",
   "id": "8f222c11f3ea5d14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Streamed response",
   "id": "5f101b51473576b6"
  },
  {
   "metadata": {
    "height": 47
   },
   "cell_type": "code",
   "source": [
    "for chunk in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(chunk, end=\"|\", flush=True)\n"
   ],
   "id": "f11f31c20841ec5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bind with parameters\n",
    "\n",
    "#### Temperature"
   ],
   "id": "6802ab45565b385d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp_model = llm.bind(temperature=1.0) | StrOutputParser()\n",
    "temp_model.invoke(\"Can you give me three great tips about what to do in Reykjavik?\")"
   ],
   "id": "60834796fde65a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Stop words",
   "id": "556df1a21a011bcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stop_model = llm.bind(stop=[\"Harpa\"]) | StrOutputParser()\n",
    "stop_model.invoke(\"Can you give me three great tips about what to do in Reykjavik?\")"
   ],
   "id": "cf9849aec1df6791",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
