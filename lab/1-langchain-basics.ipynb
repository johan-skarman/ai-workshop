{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71812daf78b1d9e1",
   "metadata": {},
   "source": [
    "# 1. LangChain basics - Messages, Prompts, Output Parsers and Chains\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/IT-HUSET/ai-workshop-250121/blob/main/lab/1-langchain-basics.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe13a58b126b36",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pydantic~=2.9 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install langchain~=0.3.7 langchain_openai~=0.2.6 langchain_community~=0.3.5 --upgrade --quiet\n",
    "%pip install langchain-anthropic~=0.3.3 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4cb397887d64",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221169915536536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a89e07a87f543d",
   "metadata": {},
   "source": [
    "### Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6077d675b1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o-mini\", temperature=0.0, openai_api_version=api_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351aa4f0adb24f4e",
   "metadata": {},
   "source": [
    "## LangChain basics\n",
    "\n",
    "This notebook introduces some of the basics concepts of LangChain.\n",
    "\n",
    "![LanChain](https://raw.githubusercontent.com/IT-HUSET/ai-workshop-250121/refs/heads/main/images/LangChain-chains.png)\n",
    "\n",
    "\n",
    "### Chain, LCEL and the Runnable interface\n",
    "\n",
    "A chain is a sequence of components with a unified interface, that are executed in order. This unified interface is called **[`Runnable`](https://python.langchain.com/docs/concepts/runnables/)** and provides common operations,  **invoking**, **streaming** and **batching** .\n",
    "\n",
    "Multiple Runnables can be composed into a chain, where the output of one Runnable is passed as input to the next Runnable in the chain. The easiest way of doing this is by using the [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/), which basically simply is some syntactic sugar that allows components to be composed together using the `|` operator.\n",
    "\n",
    "```python\n",
    "chain = runnable1 | runnable2\n",
    "```\n",
    "\n",
    "\n",
    "The output of one runnable is passed as input to the next runnable in the chain.\n",
    "https://python.langchain.com/docs/concepts/lcel/\n",
    "\n",
    "\n",
    "### Chat models\n",
    "LangChain provides a consistent interface for working with chat models from different providers. Read more [here](https://python.langchain.com/docs/concepts/chat_models/).\n",
    "\n",
    "\n",
    "### Messages\n",
    "\n",
    "Messages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
    "\n",
    "![Graph](https://github.com/IT-HUSET/ai-workshop-250121/blob/main/images/langchain-messages.png?raw=true)\n",
    "\n",
    "Read more about messages [here](https://python.langchain.com/docs/concepts/messages/).\n",
    "\n",
    "\n",
    "### Output parsing\n",
    "\n",
    "Output parsers are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\n",
    "\n",
    "Read more [here](https://python.langchain.com/docs/concepts/output_parsers/)\n",
    "\n",
    "\n",
    "### More LangChain concepts\n",
    "\n",
    "Read more about basic LangChain concepts [here](https://python.langchain.com/docs/concepts/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17a60b1998a7e6",
   "metadata": {},
   "source": [
    "## Let's start Chat models and Messages\n",
    "\n",
    "We define a system message and a human message to start a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b9ed187731be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "\n",
    "# We define a system message and a human message to start a conversation\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant, expert in Iceland tourist information.\")\n",
    "human_message = HumanMessage(content=\"Hi! I need help planning a trip to Iceland.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2c39b6954ca81",
   "metadata": {},
   "source": [
    "#### Since Chat Models are Runnables, we can invoke them using the `invoke` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c75dd8a9e44188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Of course! I'd be happy to help you plan your trip to Iceland. Here are a few questions to get us started:\\n\\n1. **Travel Dates**: When are you planning to visit Iceland?\\n2. **Duration**: How long do you intend to stay?\\n3. **Interests**: What are you most interested in seeing or doing? (e.g., nature, hiking, culture, hot springs, Northern Lights)\\n4. **Budget**: Do you have a budget in mind for accommodation, activities, and food?\\n5. **Transportation**: Are you planning to rent a car, use public transport, or join guided tours?\\n6. **Accommodation Preferences**: What type of accommodation do you prefer? (e.g., hotels, hostels, guesthouses, camping)\\n\\nOnce I have this information, I can provide you with tailored recommendations!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 34, 'total_tokens': 208, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-1c5f9c5b-80fe-4ad3-871f-541e00d16174-0' usage_metadata={'input_tokens': 34, 'output_tokens': 174, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "ai_message: AIMessage = llm.invoke([system_message, human_message])\n",
    "print(ai_message) # This will (basically) print the entire response from the LLM, including a lot of meta-data, metrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854cfa848a7e3f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! I'd be happy to help you plan your trip to Iceland. Here are a few questions to get us started:\n",
      "\n",
      "1. **Travel Dates**: When are you planning to visit Iceland?\n",
      "2. **Duration**: How long do you intend to stay?\n",
      "3. **Interests**: What are you most interested in seeing or doing? (e.g., nature, hiking, culture, hot springs, Northern Lights)\n",
      "4. **Budget**: Do you have a budget in mind for accommodation, activities, and food?\n",
      "5. **Transportation**: Are you planning to rent a car, use public transport, or join guided tours?\n",
      "6. **Accommodation Preferences**: What type of accommodation do you prefer? (e.g., hotels, hostels, guesthouses, camping)\n",
      "\n",
      "Once I have this information, I can provide you with tailored recommendations!\n"
     ]
    }
   ],
   "source": [
    "# Print just the content of the AI message\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdf2314c61ad35",
   "metadata": {},
   "source": [
    "#### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9118a0084385c73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volcanic eruptions are a natural part of Iceland's landscape, given its location on the Mid-Atlantic Ridge. While they can be concerning, here are some points to consider regarding safety and travel during such events:\n",
      "\n",
      "1. **Monitoring**: Iceland has a robust monitoring system for volcanic activity. The Icelandic Meteorological Office provides real-time updates on volcanic activity, including eruptions, ash clouds, and safety advisories.\n",
      "\n",
      "2. **Safety Protocols**: If an eruption occurs, local authorities will issue safety guidelines. It's important to follow these instructions and stay informed through official channels.\n",
      "\n",
      "3. **Travel Insurance**: Consider purchasing travel insurance that covers natural disasters. This can provide peace of mind and financial protection in case your plans are disrupted.\n",
      "\n",
      "4. **Flexibility**: If you're concerned about volcanic activity, consider keeping your itinerary flexible. This way, you can adjust your plans if necessary.\n",
      "\n",
      "5. **Popular Areas**: Some areas of Iceland are more prone to volcanic activity than others. If you're particularly worried, you might want to focus on regions that are less active, such as the Snæfellsnes Peninsula or the Westfjords.\n",
      "\n",
      "6. **Emergency Services**: Iceland has a well-prepared emergency response system. In the unlikely event of an eruption, local services will be available to assist tourists.\n",
      "\n",
      "7. **Enjoying Nature**: Remember that volcanic landscapes are part of what makes Iceland unique and beautiful. Many tourists visit to see volcanic features, such as craters, lava fields, and geothermal areas.\n",
      "\n",
      "If you have specific concerns or want to know more about certain areas or activities in relation to volcanic activity, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Let's try a follow-up question\n",
    "conversation_messages: list[BaseMessage] = [\n",
    "    system_message,\n",
    "    human_message,\n",
    "    ai_message,\n",
    "    HumanMessage(content=\"What if there's a volcanic eruption!?😱\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(conversation_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb46e69c31b014",
   "metadata": {},
   "source": [
    "Let's try with a different LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b63b354e36cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't panic! Iceland is extremely well-prepared for volcanic activity, and it's actually one of the most monitored and well-managed volcanic regions in the world. Here's what you should know:\n",
      "\n",
      "1. **Safety First**: \n",
      "- Iceland has sophisticated monitoring systems and early warning procedures\n",
      "- Local authorities provide real-time updates and clear safety guidelines\n",
      "- Tourist areas are quickly closed if there's any risk\n",
      "- Evacuations, when needed, are organized and efficient\n",
      "\n",
      "2. **Travel Impact**:\n",
      "- Most eruptions affect very limited areas\n",
      "- The majority of tourist attractions remain accessible\n",
      "- Reykjavík and most populated areas are safe\n",
      "- Air travel might be affected, but it's rare (unlike the 2010 Eyjafjallajökull eruption)\n",
      "\n",
      "3. **What to Do**:\n",
      "- Check Iceland's official Civil Protection website (www.almannavarnir.is)\n",
      "- Follow SafeTravel.is for updates\n",
      "- Register with your embassy\n",
      "- Get travel insurance that covers volcanic activity\n",
      "- Follow local authorities' guidance\n",
      "\n",
      "4. **Bonus**: \n",
      "- If safe, volcanic eruptions can actually be amazing tourist attractions!\n",
      "- Many tour operators offer special volcano viewing tours when conditions permit\n",
      "\n",
      "Would you like more specific information about current volcanic activity in Iceland or tips for preparing for your trip?\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm2 = ChatAnthropic(\n",
    "     model='claude-3-5-sonnet-20241022',\n",
    "     temperature=0.0,\n",
    ")\n",
    "\n",
    "response = llm2.invoke(conversation_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374dfefa96c8e0e",
   "metadata": {},
   "source": [
    "----\n",
    "<br/>\n",
    "\n",
    "## Prompts and Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1153c1ca06ac96",
   "metadata": {},
   "source": [
    "### Chat prompt template - for chat-based LLMs\n",
    "Basically a chat prompt template is a list of message templates. The result of invoking a chat prompt template is a `ChatPromptValue`, containing a list of messages.\n",
    "\n",
    "```python\n",
    "ChatPromptValue(\n",
    "    messages=[\n",
    "        SystemMessage(content='You are a helpful AI bot. Your name is Carl.'),\n",
    "        HumanMessage(content='Hello, there!'),\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b67cd139c66b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_template = \"Translate user input into a style that is {style}.\"\n",
    "\n",
    "# This is the easiest and most common way to create a prompt template\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{input}\"), # You can also use the alias \"user\" instead of \"human\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c3fcc82fc68e8",
   "metadata": {},
   "source": [
    "#### Let's check the messages templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a3ac2d0f7576987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style'], input_types={}, partial_variables={}, template='Translate user input into a style that is {style}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdefe947f0e4f9",
   "metadata": {},
   "source": [
    "#### Print input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92db1765d6e1812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input', 'style']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37b9f541c7aabd",
   "metadata": {},
   "source": [
    "### Using a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83125e5e1063efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "\n",
    "customer_style = \"American English in a calm and respectful tone\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_messages: ChatPromptValue = prompt_template.invoke({'style': customer_style, 'input': customer_email})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a055b1168165e9d",
   "metadata": {},
   "source": [
    "#### Let's have a look at the contents (messages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3872fdb49b8c292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.system.SystemMessage'>\n",
      "Translate user input into a style that is American English in a calm and respectful tone.\n"
     ]
    }
   ],
   "source": [
    "# First (system) message:\n",
    "print(type(customer_messages.messages[0]))\n",
    "print(customer_messages.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f992943db614e2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second (human) message:\n",
    "print(type(customer_messages.messages[1]))\n",
    "print(customer_messages.messages[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acb93ea6d27b4d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m quite frustrated that my blender lid came off and made a mess of my kitchen walls with smoothie. To add to my annoyance, the warranty doesn’t cover the cleaning costs. I would really appreciate your assistance with this situation. Thank you!\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = llm.invoke(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fe42c8338e409",
   "metadata": {},
   "source": [
    "#### Try another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20d54b3d2434415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuming, you are, hmmm? Off flew the blender lid, it did. Splattered the kitchen walls with smoothie, it has. Worse, the warranty does not cover the cleaning cost, yes. Help you, I shall, matey!\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"Speeks like yoda\"\n",
    "\n",
    "customer_messages: ChatPromptValue = prompt_template.invoke({'style': customer_style, 'input': customer_email})\n",
    "\n",
    "customer_response = llm.invoke(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71df032274e8c0",
   "metadata": {},
   "source": [
    "----\n",
    "<br/>\n",
    "\n",
    "\n",
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236fa5ba4402834f",
   "metadata": {},
   "source": [
    "## The most common parser - String output parsing\n",
    "\n",
    "Useful when you just want to extract a string (content) from the output, and not all the other metadata an LLM might return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f77f921dc68d9e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The choice between the Copenhagen interpretation and the Many-Worlds interpretation of quantum mechanics often depends on philosophical preferences and the aspects of quantum theory one finds most compelling.\\n\\n1. **Copenhagen Interpretation**: This is one of the oldest and most widely taught interpretations of quantum mechanics. It posits that quantum systems exist in a superposition of states until they are measured, at which point the wave function collapses to a definite state. This interpretation emphasizes the role of the observer and measurement in determining the state of a quantum system. It is often seen as more pragmatic, focusing on the outcomes of measurements rather than the underlying reality.\\n\\n2. **Many-Worlds Interpretation**: Proposed by Hugh Everett III in the 1950s, this interpretation suggests that all possible outcomes of quantum measurements actually occur, each in its own separate \"branch\" of the universe. In this view, there is no wave function collapse; instead, the universe continually splits into multiple, non-communicating branches. This interpretation eliminates the need for an observer-induced collapse and provides a deterministic framework, but it raises questions about the nature of reality and the meaning of probability.\\n\\nUltimately, the choice between these interpretations is largely philosophical, as both are consistent with the mathematical formalism of quantum mechanics and make the same predictions for experiments. Some physicists prefer the Copenhagen interpretation for its simplicity and practical approach, while others favor Many-Worlds for its elegant resolution of certain paradoxes. There is no consensus in the scientific community, and both interpretations continue to be subjects of debate and exploration.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 311, 'prompt_tokens': 14, 'total_tokens': 325, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-c5592b94-1853-4335-a976-eafaad823536-0' usage_metadata={'input_tokens': 14, 'output_tokens': 311, 'total_tokens': 325, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "response = llm.invoke(\"Copenhagen or Many-worlds?\")\n",
    "print(response) # Will print out the entire response, a lot of which we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58dab2c794038e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice between the Copenhagen interpretation and the Many-Worlds interpretation of quantum mechanics often depends on philosophical preferences and the aspects of quantum theory one finds most compelling.\n",
      "\n",
      "1. **Copenhagen Interpretation**: This is one of the oldest and most widely taught interpretations of quantum mechanics. It posits that quantum systems exist in a superposition of states until they are measured, at which point the wave function collapses to a definite state. This interpretation emphasizes the role of the observer and measurement in determining the state of a quantum system. It is often seen as more pragmatic, focusing on the outcomes of measurements rather than the underlying reality.\n",
      "\n",
      "2. **Many-Worlds Interpretation**: Proposed by Hugh Everett III in the 1950s, this interpretation suggests that all possible outcomes of quantum measurements actually occur, each in its own separate \"branch\" of the universe. In this view, there is no wave function collapse; instead, the universe continually splits into multiple, non-communicating branches. This interpretation eliminates the need for an observer-induced collapse and provides a deterministic framework, but it raises questions about the nature of reality and the meaning of probability.\n",
      "\n",
      "Ultimately, the choice between these interpretations is largely philosophical, as both are consistent with the mathematical formalism of quantum mechanics and make the same predictions for experiments. Some physicists prefer the Copenhagen interpretation for its simplicity and practical approach, while others favor Many-Worlds for its elegant resolution of certain paradoxes. There is no consensus in the scientific community, and both interpretations continue to be subjects of debate and exploration.\n"
     ]
    }
   ],
   "source": [
    "# Let's just extract the content\n",
    "parsed_response = str_parser.invoke(response)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd095488b3ca5c",
   "metadata": {},
   "source": [
    "## Structured output parsing\n",
    "\n",
    "![Stryctyred output](https://python.langchain.com/assets/images/structured_output-2c42953cee807dedd6e96f3e1db17f69.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661745c1102696a",
   "metadata": {},
   "source": [
    "#### This is what we'd like the output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df8870b805fae9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517db3c5da4b947",
   "metadata": {},
   "source": [
    "#### Setup the inputs and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbe9c9d9379bd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any quote about the value or price.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ef1d624b1e52f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any quote about the value or price.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82378a35fe3275",
   "metadata": {},
   "source": [
    "#### Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef70678c44331c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": \"slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n",
      "\n",
      "Response type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n",
    "print(f\"\\nResponse type: {type(response.content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10d3c0df23f733",
   "metadata": {},
   "source": [
    "### Parse the LLM output as JSON\n",
    "\n",
    "Let's fix this with proper JSON parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d75eec4c09374bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': True, 'delivery_days': 2, 'price_value': \"slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}\n",
      "\n",
      "Response type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "result = json_parser.invoke(response)\n",
    "print(result)\n",
    "print(f\"\\nResponse type: {type(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc1c3f772437ee",
   "metadata": {},
   "source": [
    "### Structured parsing with typing and optional validation (using **Pydantic**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fa89de2cb166fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gift=True delivery_days=2 price_value=\"slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "\n",
      "Response type: <class '__main__.Review'>\n",
      "Delivery days: 2\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Review(BaseModel):\n",
    "    gift: bool\n",
    "    delivery_days: int\n",
    "    price_value: str\n",
    "\n",
    "\n",
    "structured_output_llm = llm.with_structured_output(Review)\n",
    "result = structured_output_llm.invoke(messages)\n",
    "\n",
    "print(result)\n",
    "print(f\"\\nResponse type: {type(result)}\")\n",
    "print(f\"Delivery days: {result.delivery_days}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248d5bc4ed3fbcc",
   "metadata": {},
   "source": [
    "----\n",
    "<br/>\n",
    "\n",
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018a3b1f2cc174a",
   "metadata": {},
   "source": [
    "### Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fa3e907eff8d09c",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['topic'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='tell me a short joke about {topic}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# When using only a single template string, it's assumed the role is \"human\"\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "997c85d9470362fc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Build a chain (creates a RunnableSequence)\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f222c11f3ea5d14",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do bears have hairy coats? \\n\\nBecause they look silly in sweaters!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f101b51473576b6",
   "metadata": {},
   "source": [
    "### Streamed response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f11f31c20841ec5d",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||Why| do| bears| have| hairy| coats|?| \n",
      "\n",
      "|Because| they| look| silly| in| sweaters|!||"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(chunk, end=\"|\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802ab45565b385d",
   "metadata": {},
   "source": [
    "### Bind with parameters\n",
    "\n",
    "#### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60834796fde65a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Here are three great tips for things to do in Reykjavik:\\n\\n1. **Explore the Golden Circle**: While not in Reykjavik itself, the Golden Circle is a must-visit when you're in the area. This popular tourist route includes three major attractions: Þingvellir National Park, where you can see the tectonic plate boundaries; Geysir geothermal area, home to the famous Strokkur geyser that erupts every few minutes; and Gullfoss waterfall, a stunning two-tiered waterfall. Many tour operators offer day trips from Reykjavik, making it easy to experience these natural wonders.\\n\\n2. **Visit Hallgrímskirkja**: This iconic church is one of Reykjavik's most recognizable landmarks. Its unique architecture is inspired by Iceland's basalt columns, and you can take an elevator to the top for panoramic views of the city and surrounding landscape. The church's towering presence is matched by its beautiful interior, including a striking pipe organ. Don't forget to snap a photo of the statue of Leif Erikson in front of the church!\\n\\n3. **Enjoy the Harpa Concert Hall**: Located by the waterfront, Harpa is an architectural gem and a cultural hub in Reykjavik. The building's glass facade reflects the changing light of the city, creating a stunning visual experience. Check out the schedule for concerts and events, as Harpa hosts a variety of performances, from classical music to contemporary shows. Additionally, the area around Harpa offers lovely walking paths along the waterfront, perfect for a leisurely stroll or to take in the view of the dramatic landscape.\\n\\nEnjoy your time in Reykjavik!\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model = llm.bind(temperature=1.0) | StrOutputParser()\n",
    "temp_model.invoke(\"Can you give me three great tips about what to do in Reykjavik?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556df1a21a011bcd",
   "metadata": {},
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf9849aec1df6791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Absolutely! Here are three great tips for things to do in Reykjavik:\\n\\n1. **Explore the Golden Circle**: While Reykjavik itself has plenty to offer, the Golden Circle is a must-see when you're in the area. This popular route includes Þingvellir National Park, where you can see the rift between the North American and Eurasian tectonic plates, the stunning Gullfoss waterfall, and the geothermal area in Haukadalur, which contains the famous geysers Geysir and Strokkur. Many tour operators offer day trips from Reykjavik, making it easy to experience these natural wonders.\\n\\n2. **Visit the \""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_model = llm.bind(stop=[\"Harpa\"]) | StrOutputParser()\n",
    "stop_model.invoke(\"Can you give me three great tips about what to do in Reykjavik?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
