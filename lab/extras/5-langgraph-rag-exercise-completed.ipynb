{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangGraph - RAG Exercise! üöÄ\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/IT-HUSET/ai-workshop-250121/blob/main/lab/5-langgraph-rag-exercise.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>\n",
    "\n",
    "Let's apply what we've learned and build a RAG application with LangGraph and LangChain!\n",
    "\n",
    "Below you will find a partially completed notebook with some code snippets. Your task is to complete the code snippets and run the graph to retrieve relevant documents based on a user question. More specifically...\n",
    "\n",
    "### ...your task is to:\n",
    "* Load a set of documents\n",
    "* Split the documents into smaller chunks\n",
    "* Ingest the chunks into a vector database\n",
    "* Build a simple graph that retrieves relevant documents based on a user question\n",
    "\n",
    "### Additional tasks:\n",
    "* Add more documents, refine the prompts, play with different settings for chunking and retrieval etc\n",
    "* Add initial routing based on question type\n",
    "* Add grading of retrieved documents for relevance (Corrective RAG)\n"
   ],
   "id": "4c9af2cedbcec94d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "2fcf534f63f3d850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install httpx~=0.28.1 openai~=1.57 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 lark~=1.2 --upgrade --quiet\n",
    "%pip install langchain~=0.3.10 langchain_openai~=0.2.11 langchain_community~=0.3.10 langchain-chroma~=0.1.4 --upgrade --quiet\n",
    "%pip install langgraph~=0.2.56 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "de7f94673c1cabdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "3d510e51c0909d92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")"
   ],
   "id": "7085d7c17c7156a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Optional - Setup LangSmith tracing for this notebook",
   "id": "cca6a5eac7fdc81b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#import os\n",
    "\n",
    "# API key etc is in the .env file\n",
    "# my_name = \"Totoro\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"tokyo24-langgraph-{my_name}\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ],
   "id": "a2f73a0389ed5a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Chat Model",
   "id": "5b5192ae79a33f11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o-mini\", temperature=0.0, openai_api_version=api_version)\n",
    "embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_version=api_version)"
   ],
   "id": "8f6092d8a5ac3984",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup ingestion / retrieval pipeline",
   "id": "84e945bc7bc2583e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup vector DB (Chroma)",
   "id": "116bb16b204d989b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.vectorstores import VectorStore, VectorStoreRetriever\n",
    "\n",
    "persist_directory = './db/rag_exercise/'\n",
    "\n",
    "# Optionally remove the directory and all files in it recursively if it exists\n",
    "# import shutil\n",
    "# import os\n",
    "# if os.path.exists(persist_directory):\n",
    "#     shutil.rmtree(persist_directory)\n",
    "\n",
    "vectordb: Chroma = Chroma(\n",
    "    collection_name=\"rag_exercise\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory # Optionally persist the database\n",
    ")"
   ],
   "id": "9d222640ccefee9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup a text splitter",
   "id": "e3a2bb279f787706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 80\n",
    ")"
   ],
   "id": "72bdfcbd420696d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup documents to load",
   "id": "3ee64e2b269762e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Documents to load\n",
    "\n",
    "# TODO: Add your own documents here (or begin with these)\n",
    "documents_to_load = [\n",
    "    \"https://data.riksdagen.se/fil/CDA05163-DE71-448D-807D-747C997E8F3A\", # AI:s betydelse f√∂r framtidens arbetsmarknad och skola\n",
    "    \"https://data.riksdagen.se/fil/61B7540B-EEDD-4922-B61B-FC0A9F3AE4E2\", # 2024/25:263 AI, annan ny teknik och de m√§nskliga r√§ttigheterna\n",
    "    \"https://data.riksdagen.se/fil/0D43150B-5B31-43A4-89CD-4FE0478EC6C7\" # 2024/25:263 AI, annan ny teknik och de m√§nskliga r√§ttigheterna (svar)\n",
    "]"
   ],
   "id": "b5a1319121a51e71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ingest - split and add to vector index",
   "id": "6c3d3524dfc1ec9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import time\n",
    "\n",
    "def ingest_document(doc: str):\n",
    "    '''Helper function to ingest a document into the vector database'''\n",
    "\n",
    "    # Load document\n",
    "    print(f\"Loading document {doc}...\")\n",
    "    loader = PyPDFLoader(doc)\n",
    "    pages = loader.load()\n",
    "\n",
    "    # Split\n",
    "    doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "    # Add to index\n",
    "    print(f\"Adding document {doc} to index...\")\n",
    "\n",
    "    # Add to index in batches, with delay, to avoid rate limiting\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(doc_splits), batch_size):\n",
    "        batch = doc_splits[i:i + batch_size]\n",
    "        vectordb.add_documents(documents=batch)\n",
    "        print(f\"Added splits {i} to {i + batch_size}\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"Added document {doc} ({len(pages)} pages) - {len(doc_splits)} splits\")\n",
    "\n",
    "\n",
    "for doc in documents_to_load:\n",
    "    ingest_document(doc)"
   ],
   "id": "31138108665f383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup query graph / pipeline",
   "id": "fe1dc3a4f6d9fb17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Graph state",
   "id": "fa83c677f583ec19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import  List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class GraphState(MessagesState):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n"
   ],
   "id": "b9f33014df9438d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nodes",
   "id": "492b3adb4e0bee5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ],
   "id": "c86188187c933bdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retrieval (Vector Store similarity search)",
   "id": "2a970e8996661772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RetrievalNode:\n",
    "    retriever: VectorStoreRetriever\n",
    "\n",
    "    def __init__(self):\n",
    "        self.retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---RETRIEVE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        # Retrieval\n",
    "        documents = self.retriever.invoke(question)\n",
    "\n",
    "        print(f\"---RETRIEVED {len(documents)} DOCS---\")\n",
    "        #print(f\"{documents}\")\n",
    "\n",
    "        return {\"documents\": documents}"
   ],
   "id": "5370fb2ff04b4f3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### RAG Generation (LLM call with factual/grounded context)",
   "id": "59a1476bcc382c5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RAGNode:\n",
    "    system_template = \"\"\"You are an helpful assistant, expert in answering questions based on provided sources (snippets from documents) and citing the sources used to generate the answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    "    ALWAYS respond in the SAME language as the original question.\n",
    "\n",
    "    ** Context (snippets from documents): **\n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---GENERATE---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # RAG generation - setup context (i.e. relevant documents snippets)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "        # RAG generation - generate answer\n",
    "        answer = self.chain.invoke({\"question\": question, \"context\": context})\n",
    "        #print(f\"---GENERATE - ANSWER: \\n{answer}\")\n",
    "\n",
    "        return {\"documents\": documents, \"answer\": answer}"
   ],
   "id": "33440df4763d1ef3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build Graph",
   "id": "ccd3947d8187653a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Graph ####\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", RetrievalNode())  # retrieve\n",
    "workflow.add_node(\"generate\", RAGNode())  # generate\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ],
   "id": "b04bc48f05ca2985",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Use Graph",
   "id": "665e36939b57669f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"Har det i riksdagen diskuterats n√•got om risker kring anv√§ndningen av artificiell intelligens (AI)?\"\n",
    "    #\"question\": \"Vilka √§r nobelpristagarna 2024?\" # Should result in web search\n",
    "    #\"question\": \"Vad inneb√§r vitboken om artificiell intelligens?\" # Should NOT result in web search\n",
    "}\n",
    "\n",
    "# Execute graph\n",
    "result = graph.invoke(inputs)\n",
    "\n",
    "print(f\"--- ANSWER: ---\\n{result['answer']}\")"
   ],
   "id": "e13e461d67c05d10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "\n",
    "-----\n",
    "\n",
    "## Refinement - add initial routing based on question type\n",
    "\n",
    "See [here](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag) for more inspiration and guidance."
   ],
   "id": "20ed3e66505bb802"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data model for routing (structured output)",
   "id": "6e8be464207bc6d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )"
   ],
   "id": "6f3ae3212d32a943",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setup a chain for structured LLM output\n",
    "\n",
    "For simplicity, we'll make the LLM call directly in the conditional edge below. We could have introduced a separate node for this, but it would also mean we'd have to add a state field for the data source. Feel free to refactor with this improvement if you'd like."
   ],
   "id": "f1cadb21a825c382"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "routing_system_prompt = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to the swedish government system, the swedish riksdag and politics.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", routing_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "routing_chain = route_prompt | llm.with_structured_output(RouteQuery)"
   ],
   "id": "b0263ff8e0573792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conditional edge for routing",
   "id": "350745e7c84d4411"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    datasource: str = routing_chain.invoke(state[\"question\"]).datasource\n",
    "\n",
    "    if datasource == \"web_search\":\n",
    "        print(\n",
    "            \"---DECISION: WEB SEARCH---\"\n",
    "        )\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"---DECISION: RAG---\")\n",
    "        return \"vectorstore\""
   ],
   "id": "45ab7b35f78a83da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create a fake web search node",
   "id": "1b585ad141847212"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FakeWebSearchNode:\n",
    "    system_template = \"\"\"You are a helpful and cheerful assistant.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chain = self.prompt | llm.bind(temperature=1.0) | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---FAKE WEB SEARCH---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        web_results = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        print(f\"---FAKE WEB SEARCH RESULT: \\n{web_results}\")\n",
    "\n",
    "        web_results = [Document(page_content=web_results)]\n",
    "\n",
    "        return {\"documents\": web_results, \"question\": question}"
   ],
   "id": "d97cfe7e0ec8a91d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build Graph with routing",
   "id": "abc61fda221e2151"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Graph ####\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", RetrievalNode())  # retrieve\n",
    "workflow.add_node(\"generate\", RAGNode())  # generate\n",
    "workflow.add_node(\"web_search\", FakeWebSearchNode())  # web search\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ],
   "id": "7c39ef9522640690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Use Graph",
   "id": "bcde461cd38cd885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"Har det i riksdagen diskuterats n√•got om risker kring anv√§ndningen av artificiell intelligens (AI)?\"\n",
    "    #\"question\": \"Vilka var nobelpristagarna 2023?\" # Should result in web search\n",
    "    #\"question\": \"Vad inneb√§r vitboken om artificiell intelligens?\" # Should NOT result in web search\n",
    "}\n",
    "\n",
    "# Execute graph\n",
    "result = graph.invoke(inputs)\n",
    "\n",
    "print(f\"--- ANSWER: ---\\n{result['answer']}\")"
   ],
   "id": "a850c996e0713ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "\n",
    "-----\n",
    "\n",
    "## Going even further - adding grading of retrieved documents for relevance (Corrective RAG)\n",
    "\n",
    "#### Look at **`simple-rag-agent-demo.ipynb`** for inspiration - and try to implement a similar setup here."
   ],
   "id": "8fdfc3123e5cd1d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
