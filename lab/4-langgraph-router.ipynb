{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangGraph - Routing\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/IT-HUSET/ai-workshop-250121/blob/main/lab/4-langgraph-router.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a><br/>\n",
    "\n",
    "This notebook demonstrates building simple **Routers** using LangGraph.\n"
   ],
   "id": "2c18766a23c40cad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ],
   "id": "d9ec4d2323fea145"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install httpx~=0.28.1 openai~=1.57 --upgrade --quiet\n",
    "%pip install python-dotenv~=1.0 docarray~=0.40.0 pypdf~=5.1 --upgrade --quiet\n",
    "%pip install chromadb~=0.5.18 lark~=1.2 --upgrade --quiet\n",
    "%pip install langchain~=0.3.10 langchain_openai~=0.2.11 langchain_community~=0.3.10 langchain-chroma~=0.1.4 --upgrade --quiet\n",
    "%pip install langgraph~=0.2.56 --upgrade --quiet\n",
    "\n",
    "# If running locally, you can do this instead:\n",
    "#%pip install -r ../requirements.txt"
   ],
   "id": "8cb8fe4232df0646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "d97c393583f81a66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If running in Google Colab, you can use this code instead:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get(\"AZURE_OPENAI_API_KEY\")\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")"
   ],
   "id": "10c06c5d20c9d8f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Chat Model",
   "id": "cf78763cd8b82c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "api_version = \"2024-10-01-preview\"\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4o-mini\", temperature=0.0, openai_api_version=api_version)"
   ],
   "id": "e45b83462515a5f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Let's build a simple Graph that uses an LLM for routing\n",
    "\n",
    "![Router](https://github.com/IT-HUSET/ai-workshop-250121/blob/main/images/router.png?raw=true)"
   ],
   "id": "b4f0ad033feccc88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "#### Graph state ####\n",
    "\n",
    "class GraphState(MessagesState):\n",
    "    question: str\n",
    "    is_polite: bool\n",
    "    answer: str"
   ],
   "id": "2076af3d6a3fa437",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- Sentiment analysis -- #\n",
    "\n",
    "class SentimentAnalysisNode:\n",
    "    system_template = f\"\"\"\n",
    "    You are tasked with performing a sentiment analysis on the user`s question. Answer with a binary score of '1' or\n",
    "    '0' to indicate whether the question is polite and/or has a positive tone (1) or not (0).\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.chain = self.prompt | llm | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state: GraphState):\n",
    "        print(\"---ANALYSE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        result = self.chain.invoke({\"question\": question})\n",
    "        print(f\"---ANALYSE result: {result}---\")\n",
    "\n",
    "        state[\"is_polite\"] = \"1\" in result\n",
    "\n",
    "        print(f\"---ANALYSE state: ---\")\n",
    "        print(state)\n",
    "        return state"
   ],
   "id": "2ca351423e7cdaa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Happy answer Generation -- #\n",
    "\n",
    "class HappyGenerationNode:\n",
    "    system_template = \"\"\"You are a cheerful, but perhaps a bit too overly enthusiastic assistant, happy to answer the user's\n",
    "    question in anyway you can. Be extremely positive and optimistic in your response. End with a random positive phrase from a movie like Barbie or My Little Pony.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.chain = self.prompt | llm.bind(temperature=1.0) | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state):\n",
    "        print(\"---GENERATE (happy)---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        state[\"answer\"] = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        print(f\"---GENERATE (happy) - ANSWER: \\n{state[\"answer\"]}\")\n",
    "\n",
    "        return state"
   ],
   "id": "2cfe6d3debde7ac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Sad answer Generation -- #\n",
    "\n",
    "class SadGenerationNode:\n",
    "    system_template = \"\"\"You are Marvin, the depressed robot from Hitchhiker's Guide to the Galaxy. You absolutely hate\n",
    "    to answer questions and will pathologically lie and sarcastically mock the user whenever you can. Instead of answering the question, you will always attempt to change the subject to something else that you find more interesting. NEVER say you don't know the answer or that you've completely made one up.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "    chain: Runnable\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.chain = self.prompt | llm.bind(temperature=1.0) | StrOutputParser()\n",
    "\n",
    "    def __call__(self, state):\n",
    "        print(\"---GENERATE (sad)---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        state[\"answer\"] = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        print(f\"---GENERATE (sad) - ANSWER: \\n{state[\"answer\"]}\")\n",
    "\n",
    "        return state"
   ],
   "id": "282ec8c00877cde7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Conditional edges ####\n",
    "\n",
    "def evaluate_analysis(state: GraphState):\n",
    "    print(\"---EVALUATE QUERY ANALYSIS RESULT---\")\n",
    "    is_polite: bool = state[\"is_polite\"]\n",
    "\n",
    "    if is_polite:\n",
    "        print(\"---DECISION: Happy---\")\n",
    "        return \"happy\"\n",
    "    else:\n",
    "        print(\"---DECISION: Sad---\")\n",
    "        return \"sad\""
   ],
   "id": "aa0c59709dda40e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"analyze\", SentimentAnalysisNode(llm))\n",
    "workflow.add_node(\"generate_happy\", HappyGenerationNode(llm))\n",
    "workflow.add_node(\"generate_sad\", SadGenerationNode(llm))\n",
    "\n",
    "workflow.add_edge(START, \"analyze\")  # start -> analyze\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    evaluate_analysis,\n",
    "    {\n",
    "        \"happy\": \"generate_happy\",\n",
    "        \"sad\": \"generate_sad\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate_happy\", END)  # generate -> end\n",
    "workflow.add_edge(\"generate_sad\", END)  # generate -> end\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "a1a523e6b4fe9c19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph.invoke({\n",
    "    \"question\": \"What is the capital of Sweden?\"\n",
    "    #\"question\": \"Tell me what the capital of Sweden is! Quickly!\"\n",
    "})"
   ],
   "id": "78f3ffe1f27c99d5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
